import streamlit as st  # importa a biblioteca Streamlit para criar a interface web

import time             # importa o mÃ³dulo time (nÃ£o utilizado diretamente, mas mantido caso queira adicionar delays)

from langchain_community.document_loaders import PyPDFLoader # importa o carregador de PDFs da LangChain Community
                         
from langchain.text_splitter import RecursiveCharacterTextSplitter  # importa a classe para dividir o texto em pedaÃ§os menores
                         
from langchain_google_genai import GoogleGenerativeAIEmbeddings  # importa o gerador de embeddings da API Generative AI do Google
                         
from langchain_chroma import Chroma   # importa o armazenamento vetorial Chroma para indexaÃ§Ã£o e busca
                                   
from langchain_google_genai import ChatGoogleGenerativeAI # importa o wrapper da LangChain para chat usando o modelo Generative AI do Google
                         
from langchain.chains import create_retrieval_chain  # importa funÃ§Ã£o para criar uma cadeia de recuperaÃ§Ã£o (RAG)
                         
from langchain.chains.combine_documents import create_stuff_documents_chain  # importa funÃ§Ã£o para combinar documentos recuperados em um Ãºnico prompt
                         
from langchain_core.prompts import ChatPromptTemplate  # importa a classe para criar templates de prompt para chat
                         
from dotenv import load_dotenv  # importa funÃ§Ã£o para carregar variÃ¡veis de ambiente de um arquivo .env

import glob

import json

import warnings
warnings.filterwarnings("ignore")  # ignora avisos de depreciaÃ§Ã£o e outros avisos

# â”€â”€â”€ CONFIGURAÃ‡ÃƒO STREAMLIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â¬‡ï¸ CONFIGURAÃ‡ÃƒO DA PÃGINA (TÃTULO E ÃCONE DA ABA) â¬‡ï¸
st.set_page_config(
    page_title="Sobre a Banda PoliRockers",    # tÃ­tulo que aparece na aba do navegador
    page_icon="ğŸ“š"                         # emoji como favicon; pode ser caminho "favicon.png"
    # layout="wide"                          # opcional, deixa a pÃ¡gina em modo â€œwideâ€
)

load_dotenv()                    # carrega as variÃ¡veis definidas em .env para o ambiente


# â”€â”€â”€ CARREGAMENTO DE DOCUMENTOS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


# Define o tÃ­tulo da aplicaÃ§Ã£o que serÃ¡ exibido no topo da interface Streamlit
st.title("Banda PoliRockers")

# 1) Encontre todos os PDFs em docs/
# pdf_paths = glob.glob("/mnt/A06815BA68159060/FLISOL/2025/docs/*.pdf")

################### Para utilizar apenas com um Pdf Especifico ########################
# Carrega o PDF especificado a partir do caminho local
loader = PyPDFLoader("/mnt/A06815BA68159060/FLISOL/2025/docs/polirockers.pdf")
data = loader.load()  # realiza a leitura do PDF e retorna uma lista de objetos de pÃ¡gina

# 2) Carregue e agregue todas as pÃ¡ginas de todos os PDFs
# all_pages = []
# for path in pdf_paths:
#     loader = PyPDFLoader(path)
#     pages = loader.load()        # retorna lista de pÃ¡ginas do PDF atual
#     all_pages.extend(pages)      # adiciona ao conjunto geral


# Configura o splitter para dividir o texto em pedaÃ§os de atÃ© 1000 caracteres, mantendo contexto
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)
docs = text_splitter.split_documents(data) # Caso utilize apenas um pdf
# docs = text_splitter.split_documents(all_pages)
# docs agora Ã© uma lista de "documentos" menores, prontos para indexaÃ§Ã£o


# Cria o armazenamento vetorial (vector store) a partir dos documentos fragmentados
# Utiliza embeddings gerados pelo modelo "embedding-001" da Google Generative AI
vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=GoogleGenerativeAIEmbeddings(model="models/embedding-001"),
    # persist_directory="./chroma_db"    # â† cria ./chroma_db e o tenant default_tenant automaticamente
)

# Cria um retriever que buscarÃ¡ os trechos mais similares no vectorstore
# Configura para retornar os top 10 resultados por similaridade
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 10})


# Inicializa o LLM de chat, apontando para o modelo "gemini-1.5-flash-latest"
# Define temperatura zero para respostas determinÃ­sticas e sem limite de tokens
llm = ChatGoogleGenerativeAI(
    model="models/gemini-1.5-flash-latest",
    temperature=0,
    max_tokens=None,
    timeout=None
)


# Cria um campo de input de chat na interface; o conteÃºdo digitado pelo usuÃ¡rio vai para "query" - BKP
query = st.chat_input("Faz teu nome: ")
prompt = query  # variÃ¡vel temporÃ¡ria que receberÃ¡ o prompt do usuÃ¡rio


# Define o prompt de sistema (system message) que orienta o comportamento do assistente
system_prompt = (
    "VocÃª Ã© um assistente de perguntas e respostas. Utilize os trechos de contexto recuperado abaixo para responder Ã  pergunta. Se nÃ£o souber a resposta, diga que nÃ£o sabe, ignore todo e qualquer questÃ£o fora do contexto dos dados. Use, no mÃ¡ximo, trÃªs frases e mantenha a resposta concisa."
    "\n\n"
    "{context}"  # placeholder que serÃ¡ preenchido com os trechos recuperados
)


# ConstrÃ³i um template de prompt para chat, mesclando a system message e a mensagem humana
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),  # define instruÃ§Ãµes de sistema
        ("human", "{input}"),       # define onde entra a pergunta do usuÃ¡rio
    ]
)

# â”€â”€â”€ SESSÃƒO PARA HISTÃ“RICO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "history" not in st.session_state:
    st.session_state.history = []  # cada item serÃ¡ dict: {"user": ..., "bot": ...}
    
# â”€â”€â”€ INPUT DE USUÃRIO E RESPOSTA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# query = st.chat_input("FaÃ§a sua pergunta:")
# if query:
#     qa_chain = create_stuff_documents_chain(llm, prompt)
#     rag_chain = create_retrieval_chain(retriever, qa_chain)
#     result = rag_chain.invoke({"input": query})
#     answer = result["answer"]
#     # salva no histÃ³rico
#     st.session_state.history.append({"user": query, "bot": answer})

# Quando o usuÃ¡rio enviar algo no chat... # BKP
if query:
    # Cria uma subcadeia que combina documentos recuperados em um Ãºnico prompt
    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    # Cria a cadeia completa de RAG: busca no retriever + passa para a subcadeia Q&A
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    # Invoca a cadeia com a entrada do usuÃ¡rio
    response = rag_chain.invoke({"input": query})
    # response["answer"] contÃ©m a resposta gerada pelo modelo

    # Exibe a resposta gerada na interface Streamlit
    st.write(response["answer"])

# â”€â”€â”€ EXIBE HISTÃ“RICO NA TELA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# st.markdown("#### HistÃ³rico de Conversa")
# for entry in st.session_state.history:
#     st.markdown(f"**VocÃª:** {entry['user']}")
#     st.markdown(f"**Assistente:** {entry['bot']}")
#     st.write("---")

# â”€â”€â”€ DOWNLOAD DO HISTÃ“RICO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# if st.session_state.history:
#     history_json = json.dumps(st.session_state.history, ensure_ascii=False, indent=2)
#     st.download_button(
#         label="ğŸ“¥ Baixar HistÃ³rico",
#         data=history_json,
#         file_name="chat_history.json",
#         mime="application/json"
#     )